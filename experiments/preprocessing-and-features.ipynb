{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import spacy\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "sys.path.append('..')\n",
    "from utils import cv_kfold, train_validate_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_file(filename: str) -> pd.DataFrame:\n",
    "    return pd.DataFrame([\n",
    "        (l.split()[0], l.split()[1], ' '.join(l.split()[3:]))\n",
    "        for l in open(filename, encoding='utf8')\n",
    "        ], columns=['class', 'sent', 'text']\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train = read_file('../datasets/train.txt')\n",
    "df_test = read_file('../datasets/test.txt')\n",
    "\n",
    "len(df_train), len(df_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = df_train['text'].values\n",
    "y = df_train['class'].values\n",
    "y_sent = df_train['sent'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Features pipelines"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TextLength(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([[len(i)] for i in X])\n",
    "\n",
    "class WordCount(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([[len(i.split())] for i in X])\n",
    "    \n",
    "class AverageTokensLength(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([[np.mean([len(j) for j in i.split()])] for i in X])\n",
    "\n",
    "\n",
    "class PredictSentiment(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, X, y_sent):\n",
    "        self.text_to_sentiment = {i: sent for i, sent in zip(X, y_sent)}\n",
    "        self.sent_to_num = {'pos': 1, 'neg': -1, '': 0}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            [self.sent_to_num[self.text_to_sentiment[i]]] for i in X\n",
    "        ])\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "def spacy_tokenizer(doc):\n",
    "    return [x.orth_ for x in nlp(doc)]\n",
    "\n",
    "def spacy_lemmatizer(doc):\n",
    "    return [x.lemma_ for x in nlp(doc)]\n",
    "\n",
    "def nltk_stems(doc):\n",
    "    return [stemmer.stem(x) for x in doc]\n",
    "\n",
    "def spacy_pos(doc):\n",
    "    return [token.pos_ for token in nlp(doc)]\n",
    "\n",
    "def spacy_ne(doc):\n",
    "    return [ent.label_ for ent in nlp(doc).ents]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_base_model = ('clf', LinearSVC())\n",
    "_base_vec = ('vec', TfidfVectorizer())\n",
    "\n",
    "features = {\n",
    "    'Baseline': Pipeline([_base_vec, _base_model]),\n",
    "    'Text Length': Pipeline([('feat', FeatureUnion([_base_vec, ('len', TextLength())])), _base_model]),\n",
    "    'Avg Tokens Length': Pipeline([('feat', FeatureUnion([_base_vec, ('avglen', AverageTokensLength())])), _base_model]),\n",
    "    'Word Count':Pipeline([('feat', FeatureUnion([_base_vec, ('wcnt', WordCount())])), _base_model]),\n",
    "    'Sentiment': Pipeline([('feat', FeatureUnion([_base_vec, ('sent', PredictSentiment(X, y_sent))])), _base_model]),\n",
    "    'StopWords Filter': Pipeline([('feat', FeatureUnion([('vec', TfidfVectorizer(stop_words='english'))])), _base_model]),\n",
    "    'NGrams 1-3': Pipeline([('feat', FeatureUnion([('vec', TfidfVectorizer(ngram_range=(1, 3)))])), _base_model]),\n",
    "    'Add CountVectorizer': Pipeline([('feat', FeatureUnion([_base_vec, ('vec2', CountVectorizer())])), _base_model]),\n",
    "    'Spacy Tokenizer': Pipeline([('feat', FeatureUnion([('vec', TfidfVectorizer(tokenizer=spacy_tokenizer))])), _base_model]),\n",
    "    'Spacy Lemmatizer': Pipeline([('feat', FeatureUnion([('vec', TfidfVectorizer(tokenizer=spacy_lemmatizer))])), _base_model]),\n",
    "    'NGrams 1-2 + StopWords Filter + sublinear_tf=True': Pipeline([('feat', FeatureUnion([('vec', TfidfVectorizer(stop_words=\"english\",sublinear_tf=True,ngram_range=(1,2)))])), _base_model]),\n",
    "    'Character-level': Pipeline([('feat', FeatureUnion([('vec', TfidfVectorizer(analyzer='char_wb', ngram_range=(3, 8), min_df=3))])), _base_model]),\n",
    "    'NLTK Stemming': Pipeline([('feat', FeatureUnion([('vec', TfidfVectorizer(tokenizer=nltk_stems))])), _base_model]),\n",
    "    'Lemmatizing + POS Tags': Pipeline([('feat', FeatureUnion([('vec', TfidfVectorizer(tokenizer=spacy_lemmatizer)), ('posvec', CountVectorizer(tokenizer=spacy_pos))])), _base_model]),\n",
    "    'Lemmatizing + NE Tags': Pipeline([('feat', FeatureUnion([('vec', TfidfVectorizer(tokenizer=spacy_lemmatizer)), ('nevec', CountVectorizer(tokenizer=spacy_ne))])), _base_model]),\n",
    "    'Lemmatizing + POS Tags + NE Tags':Pipeline([('feat', FeatureUnion([('vec', TfidfVectorizer(tokenizer=spacy_lemmatizer)), ('nevec', CountVectorizer(tokenizer=spacy_ne)), ('posvec', CountVectorizer(tokenizer=spacy_ne))])), _base_model]),\n",
    "    'Lemmatizing + Text Length + Avg Token Length + Word Count':Pipeline([('feat', FeatureUnion([('vec', TfidfVectorizer(tokenizer=spacy_lemmatizer)), ('num_feat_sc', Pipeline([('num_feat', FeatureUnion([('len', TextLength()), ('avglen', AverageTokensLength()), ('wcnt', WordCount())])), ('sc', StandardScaler())]))])), _base_model])\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(columns=['score', 'time'], index=features.keys())\n",
    "scorer = lambda *x: metrics.f1_score(*x, average='micro')\n",
    "\n",
    "for features_name, features_pipeline in tqdm(features.items(), total=len(features), desc='Pipelines'):\n",
    "    try:\n",
    "        kfold_result = cv_kfold(features_pipeline, X, y, scorer=scorer, k=5)\n",
    "        df_scores.loc[features_name, 'score'] = kfold_result['oof_score']\n",
    "        df_scores.loc[features_name, 'time'] = kfold_result['mean_time']\n",
    "\n",
    "        # X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "        # validate_results = train_validate_split(features_pipeline, X_train, y_train, X_val, y_val, scorer, verbose=0)\n",
    "        # df_scores.loc[features_name, 'score'] = validate_results['score']\n",
    "        # df_scores.loc[features_name, 'time'] = validate_results['time']\n",
    "    except Exception as e:\n",
    "        df_scores.loc[features_name, 'score'] = None\n",
    "        df_scores.loc[features_name, 'time'] = None\n",
    "\n",
    "df_scores['diff'] = df_scores['score'] - df_scores.loc['Baseline', 'score']\n",
    "\n",
    "df_scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}